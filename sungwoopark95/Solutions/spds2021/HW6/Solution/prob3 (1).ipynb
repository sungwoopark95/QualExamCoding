{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "prob3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0nrNtQeBndA"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrhGenzMBqNT",
        "outputId": "7ae4fd8f-3f78-4e97-f9cf-3b4510b0054d"
      },
      "source": [
        "#import the nescessary libs\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Loading the Fashion-MNIST dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Get GPU Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5,), (0.5,))\n",
        "                                                                   ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
        "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n",
        "\n",
        "# Examine a sample\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 10),\n",
        "                      #nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 5\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        ps = model(images)\n",
        "        test_loss += criterion(ps, labels)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5.. Training loss: 0.486.. Test loss: 0.432.. Test Accuracy: 0.841\n",
            "Epoch: 2/5.. Training loss: 0.373.. Test loss: 0.400.. Test Accuracy: 0.853\n",
            "Epoch: 3/5.. Training loss: 0.336.. Test loss: 0.392.. Test Accuracy: 0.859\n",
            "Epoch: 4/5.. Training loss: 0.314.. Test loss: 0.427.. Test Accuracy: 0.851\n",
            "Epoch: 5/5.. Training loss: 0.299.. Test loss: 0.365.. Test Accuracy: 0.871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJwM_ThzEAVA",
        "outputId": "cb4a7875-4972-415e-fbe5-a8213f5deb76"
      },
      "source": [
        "#import the nescessary libs\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Loading the Fashion-MNIST dataset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Get GPU Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5,), (0.5,))\n",
        "                                                                   ])\n",
        "# Download and load the training data\n",
        "trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n",
        "testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n",
        "\n",
        "# Examine a sample\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# Define the network architecture\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model = nn.Sequential(nn.Linear(784, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128, 10),\n",
        "                      #nn.LogSoftmax(dim = 1)\n",
        "                     )\n",
        "model.to(device)\n",
        "\n",
        "# Define the loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
        "\n",
        "# Define the epochs\n",
        "epochs = 5\n",
        "\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "for e in range(epochs):\n",
        "  running_loss = 0\n",
        "  for images, labels in trainloader:\n",
        "    # Flatten Fashion-MNIST images into a 784 long vector\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    images = images.view(images.shape[0], -1)\n",
        "    \n",
        "    # Training pass\n",
        "    #optimizer.zero_grad()\n",
        "    \n",
        "    output = model.forward(images)\n",
        "    loss = criterion(output, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    test_loss = 0\n",
        "    accuracy = 0\n",
        "    \n",
        "    # Turn off gradients for validation, saves memory and computation\n",
        "    with torch.no_grad():\n",
        "      # Set the model to evaluation mode\n",
        "      model.eval()\n",
        "      \n",
        "      # Validation pass\n",
        "      for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        ps = model(images)\n",
        "        test_loss += criterion(ps, labels)\n",
        "        top_p, top_class = ps.topk(1, dim = 1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    \n",
        "    model.train()\n",
        "\n",
        "    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n",
        "          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n",
        "          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n",
        "          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5.. Training loss: 2.315.. Test loss: 2.668.. Test Accuracy: 0.229\n",
            "Epoch: 2/5.. Training loss: 2.715.. Test loss: 2.805.. Test Accuracy: 0.195\n",
            "Epoch: 3/5.. Training loss: 2.899.. Test loss: 3.302.. Test Accuracy: 0.233\n",
            "Epoch: 4/5.. Training loss: 4.155.. Test loss: 3.258.. Test Accuracy: 0.178\n",
            "Epoch: 5/5.. Training loss: 2.956.. Test loss: 3.321.. Test Accuracy: 0.118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoweHzceMZvw"
      },
      "source": [
        "In lecture, we have learned that the place where the gradient resets lies in optimizer.zero_grad(). The following function will make the gradient to be zero. If the function is not called, the gradient will be accumulated throughtout the functions. Before making any changes, I have obtained the following result:\n",
        "- Epoch: 1/5\n",
        "  - Training loss: 0.486\n",
        "  - Test loss: 0.432\n",
        "  - Test Accuracy: 0.841\n",
        "- Epoch: 2/5\n",
        "  - Training loss: 0.373\n",
        "  - Test loss: 0.400.. Test Accuracy: 0.853\n",
        "- Epoch: 3/5\n",
        "  - Training loss: 0.336\n",
        "  - Test loss: 0.392\n",
        "  - Test Accuracy: 0.859\n",
        "- Epoch: 4/5\n",
        "  - Training loss: 0.314\n",
        "  - Test loss: 0.427\n",
        "  - Test Accuracy: 0.851\n",
        "- Epoch: 5/5 \n",
        "  - Training loss: 0.299\n",
        "  - Test loss: 0.365\n",
        "  - Test Accuracy: 0.871\n",
        "\n",
        "Running the code without optimizer.zero_grad, I have obtained the following result:\n",
        "- Epoch: 1/5\n",
        "  - Training loss: 2.315\n",
        "  - Test loss: 2.668\n",
        "  - Test Accuracy: 0.229\n",
        "- Epoch: 2/5\n",
        "  - Training loss: 2.715\n",
        "  - Test loss: 2.805\n",
        "  - Test Accuracy: 0.195\n",
        "- Epoch: 3/5\n",
        "  - Training loss: 2.899\n",
        "  - Test loss: 3.302\n",
        "  - Test Accuracy: 0.233\n",
        "- Epoch: 4/5\n",
        "  - Training loss: 4.155\n",
        "  - Test loss: 3.258\n",
        "  - Test Accuracy: 0.178\n",
        "- Epoch: 5/5\n",
        "  - Training loss: 2.956\n",
        "  - Test loss: 3.321\n",
        "  - Test Accuracy: 0.118\n",
        "\n",
        "Comparing the two, one with the optimizer.zero_grad actually has performed much better. While one with optimizer.zero_grad has shown some type of pattern where increasing the epoch increase the test accuracy (for most case). On the other hand, one without the zero_grad appears to have decrease in test accuracy as the number of epoch increases."
      ]
    }
  ]
}