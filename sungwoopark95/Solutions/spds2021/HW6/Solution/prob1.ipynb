{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prob1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPVZ6Ao2UGeUvM4Oaf6yakk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lxeRHoAx2rid"},"source":["# Porblem 1"]},{"cell_type":"code","metadata":{"id":"CH_J-X4s1y_n","executionInfo":{"status":"ok","timestamp":1637136530492,"user_tz":-540,"elapsed":327,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}}},"source":["import time"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tay_8NQY2yeV"},"source":["## Run the Script as it is"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Lvu2Idf5m9W","executionInfo":{"status":"ok","timestamp":1637136649803,"user_tz":-540,"elapsed":119061,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"38ea33e7-4ec3-4bcc-991c-a363d67f9c8e"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["19.77926993370056\n","Epoch: 1/5.. Training loss: 0.488.. Test loss: 0.423.. Test Accuracy: 0.840\n","19.79761290550232\n","Epoch: 2/5.. Training loss: 0.373.. Test loss: 0.396.. Test Accuracy: 0.854\n","20.1186466217041\n","Epoch: 3/5.. Training loss: 0.339.. Test loss: 0.373.. Test Accuracy: 0.866\n","20.131465196609497\n","Epoch: 4/5.. Training loss: 0.316.. Test loss: 0.361.. Test Accuracy: 0.872\n","20.082324266433716\n","Epoch: 5/5.. Training loss: 0.297.. Test loss: 0.371.. Test Accuracy: 0.867\n","118.79080986976624\n"]}]},{"cell_type":"markdown","metadata":{"id":"b3iKSK4g6UwA"},"source":["## Batch Size 16"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KO0qF1Ac6BcD","executionInfo":{"status":"ok","timestamp":1637136813651,"user_tz":-540,"elapsed":163854,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"13e8b83c-e74c-4e50-d91f-fb76c5c79031"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 16, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 16, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["27.752448081970215\n","Epoch: 1/5.. Training loss: 0.476.. Test loss: 0.448.. Test Accuracy: 0.837\n","28.701561212539673\n","Epoch: 2/5.. Training loss: 0.375.. Test loss: 0.404.. Test Accuracy: 0.855\n","29.306257724761963\n","Epoch: 3/5.. Training loss: 0.342.. Test loss: 0.390.. Test Accuracy: 0.860\n","28.63768243789673\n","Epoch: 4/5.. Training loss: 0.317.. Test loss: 0.367.. Test Accuracy: 0.868\n","28.957837343215942\n","Epoch: 5/5.. Training loss: 0.303.. Test loss: 0.361.. Test Accuracy: 0.874\n","163.80136346817017\n"]}]},{"cell_type":"markdown","metadata":{"id":"jvCDl-lc6YYC"},"source":["## Batch Size 64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2tr3Z-8b6aCC","executionInfo":{"status":"ok","timestamp":1637136911017,"user_tz":-540,"elapsed":97369,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"1f7431a1-f68a-4f42-e308-8a4a12f9240f"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["16.528880834579468\n","Epoch: 1/5.. Training loss: 0.500.. Test loss: 0.440.. Test Accuracy: 0.843\n","16.736101388931274\n","Epoch: 2/5.. Training loss: 0.382.. Test loss: 0.427.. Test Accuracy: 0.849\n","16.673275470733643\n","Epoch: 3/5.. Training loss: 0.347.. Test loss: 0.371.. Test Accuracy: 0.863\n","16.656588792800903\n","Epoch: 4/5.. Training loss: 0.319.. Test loss: 0.355.. Test Accuracy: 0.873\n","16.606858253479004\n","Epoch: 5/5.. Training loss: 0.300.. Test loss: 0.363.. Test Accuracy: 0.867\n","97.05978083610535\n"]}]},{"cell_type":"markdown","metadata":{"id":"VA3jeTd66aLG"},"source":["## Learning Rate 0.01"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X_XgYzOP6aQR","executionInfo":{"status":"ok","timestamp":1637137030463,"user_tz":-540,"elapsed":119448,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"caea6591-7cc5-4e47-c0bd-6a8058b49acf"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = 0.01)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["20.39034152030945\n","Epoch: 1/5.. Training loss: 0.583.. Test loss: 0.535.. Test Accuracy: 0.817\n","20.72736096382141\n","Epoch: 2/5.. Training loss: 0.503.. Test loss: 0.512.. Test Accuracy: 0.826\n","20.818732261657715\n","Epoch: 3/5.. Training loss: 0.486.. Test loss: 0.509.. Test Accuracy: 0.835\n","20.53863549232483\n","Epoch: 4/5.. Training loss: 0.473.. Test loss: 0.548.. Test Accuracy: 0.823\n","20.832820177078247\n","Epoch: 5/5.. Training loss: 0.456.. Test loss: 0.510.. Test Accuracy: 0.834\n","119.51724076271057\n"]}]},{"cell_type":"markdown","metadata":{"id":"uOt1E30d6c2x"},"source":["## Learning Rate 0.0001"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UY3bZT-e6-Yi","executionInfo":{"status":"ok","timestamp":1637137148616,"user_tz":-540,"elapsed":118157,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"7cacf694-4aad-47a6-bf85-ab03ecbd2c2e"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["20.479434728622437\n","Epoch: 1/5.. Training loss: 0.651.. Test loss: 0.514.. Test Accuracy: 0.815\n","20.618394374847412\n","Epoch: 2/5.. Training loss: 0.455.. Test loss: 0.460.. Test Accuracy: 0.835\n","20.535950422286987\n","Epoch: 3/5.. Training loss: 0.416.. Test loss: 0.441.. Test Accuracy: 0.841\n","20.329606533050537\n","Epoch: 4/5.. Training loss: 0.392.. Test loss: 0.419.. Test Accuracy: 0.848\n","20.27568006515503\n","Epoch: 5/5.. Training loss: 0.374.. Test loss: 0.409.. Test Accuracy: 0.853\n","118.01114583015442\n"]}]},{"cell_type":"markdown","metadata":{"id":"65g07IMa6dAC"},"source":["## Optimizer SGD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ziolEzCS6dD0","executionInfo":{"status":"ok","timestamp":1637137260866,"user_tz":-540,"elapsed":112256,"user":{"displayName":"­문정현 / 학생 / 데이터사이언스학과","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12935964920098229097"}},"outputId":"a63ac3be-b890-495a-d7e8-a5e000a0f5da"},"source":["time1 = time.time()\n","#import the nescessary libs\n","import numpy as np\n","import torch\n","\n","# Loading the Fashion-MNIST dataset\n","from torchvision import datasets, transforms\n","\n","# Get GPU Device\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","# Define a transform to normalize the data\n","transform = transforms.Compose([transforms.ToTensor(),\n","                                    transforms.Normalize((0.5,), (0.5,))\n","                                                                   ])\n","# Download and load the training data\n","trainset = datasets.FashionMNIST('MNIST_data/', download = True, train = True, transform = transform)\n","testset = datasets.FashionMNIST('MNIST_data/', download = True, train = False, transform = transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n","testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n","\n","# Examine a sample\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","\n","# Define the network architecture\n","from torch import nn, optim\n","import torch.nn.functional as F\n","\n","model = nn.Sequential(nn.Linear(784, 128),\n","                      nn.ReLU(),\n","                      nn.Linear(128, 10),\n","                      #nn.LogSoftmax(dim = 1)\n","                     )\n","model.to(device)\n","\n","# Define the loss\n","criterion = nn.CrossEntropyLoss()\n","\n","# Define the optimizer\n","optimizer = optim.SGD(model.parameters(), lr = 0.001)\n","\n","# Define the epochs\n","epochs = 5\n","\n","train_losses, test_losses = [], []\n","\n","for e in range(epochs):\n","  running_loss = 0\n","  train_time = time.time()\n","  for images, labels in trainloader:\n","    # Flatten Fashion-MNIST images into a 784 long vector\n","    images = images.to(device)\n","    labels = labels.to(device)\n","    images = images.view(images.shape[0], -1)\n","    \n","    # Training pass\n","    optimizer.zero_grad()\n","    \n","    output = model.forward(images)\n","    loss = criterion(output, labels)\n","    loss.backward()\n","    optimizer.step()\n","    \n","    running_loss += loss.item()\n","  else:\n","    train_time2 = time.time()\n","    print(str(train_time2 - train_time))\n","    test_loss = 0\n","    accuracy = 0\n","    \n","    # Turn off gradients for validation, saves memory and computation\n","    with torch.no_grad():\n","      # Set the model to evaluation mode\n","      model.eval()\n","      \n","      # Validation pass\n","      for images, labels in testloader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        images = images.view(images.shape[0], -1)\n","        ps = model(images)\n","        test_loss += criterion(ps, labels)\n","        top_p, top_class = ps.topk(1, dim = 1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        accuracy += torch.mean(equals.type(torch.FloatTensor))\n","    \n","    model.train()\n","\n","    print(\"Epoch: {}/{}..\".format(e+1, epochs),\n","          \"Training loss: {:.3f}..\".format(running_loss/len(trainloader)),\n","          \"Test loss: {:.3f}..\".format(test_loss/len(testloader)),\n","          \"Test Accuracy: {:.3f}\".format(accuracy/len(testloader)))\n","\n","time2 = time.time()\n","print(str(time2 - time1))"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]},{"output_type":"stream","name":"stdout","text":["19.296220779418945\n","Epoch: 1/5.. Training loss: 1.355.. Test loss: 0.901.. Test Accuracy: 0.726\n","19.082611083984375\n","Epoch: 2/5.. Training loss: 0.774.. Test loss: 0.716.. Test Accuracy: 0.756\n","19.03814721107483\n","Epoch: 3/5.. Training loss: 0.661.. Test loss: 0.647.. Test Accuracy: 0.772\n","19.375463485717773\n","Epoch: 4/5.. Training loss: 0.607.. Test loss: 0.608.. Test Accuracy: 0.780\n","19.402095079421997\n","Epoch: 5/5.. Training loss: 0.572.. Test loss: 0.580.. Test Accuracy: 0.795\n","112.22894525527954\n"]}]},{"cell_type":"markdown","metadata":{"id":"-EybI2Wb6dHT"},"source":["## Overall Result"]},{"cell_type":"markdown","metadata":{"id":"FzFb88116omi"},"source":["In order to run the code in appropriate manner, I have started with Script as it is with including the running time. From the result, I have obtained the following\n","- Epoch: 1/5.\n","  - Training loss: 0.488\n","  - Test loss: 0.423\n","  - Test Accuracy: 0.840\n","  - Time: 19.77926993370056\n","- Epoch: 2/5.\n","  - Training loss: 0.373\n","  - Test loss: 0.396\n","  - Test Accuracy: 0.854\n","  - Time: 19.79761290550232\n","- Epoch: 3/5.\n","  - Training loss: 0.339\n","  - Test loss: 0.373\n","  - Test Accuracy: 0.866\n","  - Time: 20.1186466217041\n","- Epoch: 4/5.\n","  - Training loss: 0.316\n","  - Test loss: 0.361\n","  - Test Accuracy: 0.872\n","  - Time: 20.131465196609497\n","- Epoch: 5/5.\n","  - Training loss: 0.297\n","  - Test loss: 0.371\n","  - Test Accuracy: 0.867\n","  - Time: 20.082324266433716\n","- Total time: 118.79080986976624\n","\n","For the Batch size, I have changed following items:\n","- trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32, shuffle = True, num_workers=4)\n","- testloader = torch.utils.data.DataLoader(testset, batch_size = 32, shuffle = True, num_workers=4)\n","- From the two codes, I have changed the batch_size from 32 to 16 and 64.\n","\n","With the following changes, I have obtained the following result for batch size of 16 to be as follows: \n","- Epoch: 1/5.\n","  - Training loss: 0.476\n","  - Test loss: 0.448\n","  - Test Accuracy: 0.837\n","  - Time: 27.752448081970215\n","- Epoch: 2/5.\n","  - Training loss: 0.375\n","  - Test loss: 0.404\n","  - Test Accuracy: 0.855\n","  - Time: 28.701561212539673\n","- Epoch: 3/5.\n","  - Training loss: 0.342\n","  - Test loss: 0.390\n","  - Test Accuracy: 0.860\n","  - Time: 29.306257724761963\n","- Epoch: 4/5.\n","  - Training loss: 0.317\n","  - Test loss: 0.367\n","  - Test Accuracy: 0.868\n","  - Time: 28.63768243789673\n","- Epoch: 5/5.\n","  - Training loss: 0.303\n","  - Test loss: 0.361\n","  - Test Accuracy: 0.874\n","  - Time: 28.957837343215942\n","- Total time: 163.80136346817017\n","\n","For the case of batch size of 64, I have obtained:\n","- Epoch: 1/5.\n","  - Training loss: 0.500\n","  - Test loss: 0.440\n","  - Test Accuracy: 0.843\n","  - Time: 16.528880834579468\n","- Epoch: 2/5.\n","  - Training loss: 0.382\n","  - Test loss: 0.427\n","  - Test Accuracy: 0.849\n","  - Time: 16.736101388931274\n","- Epoch: 3/5.\n","  - Training loss: 0.347\n","  - Test loss: 0.371\n","  - Test Accuracy: 0.863\n","  - Time: 16.673275470733643\n","- Epoch: 4/5.\n","  - Training loss: 0.319\n","  - Test loss: 0.355\n","  - Test Accuracy: 0.873\n","  - Time: 16.656588792800903\n","- Epoch: 5/5.\n","  - Training loss: 0.300\n","  - Test loss: 0.363\n","  - Test Accuracy: 0.867\n","  - Time: 16.606858253479004\n","- Total time: 97.05978083610535\n","\n","As one can see from the above result, as the batch size increases, the time to train and run the entire script decreases. However. it appears as if changning the batch size does not seems to affect the test accuracy. Considering training loss and test loss, there appears to be no impact.\n","\n","For the Learning Rate, the change occurred in \n","- optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","- From above, I have changed lr to be 0.01 and 0.0001\n","\n","With the following changes, I have obtained the following result for lr = 0.01 to be as follows: \n","- Epoch: 1/5.\n","  - Training loss: 0.583\n","  - Test loss: 0.535\n","  - Test Accuracy: 0.817\n","  - Time: 20.39034152030945\n","- Epoch: 2/5.\n","  - Training loss: 0.503\n","  - Test loss: 0.512\n","  - Test Accuracy: 0.826\n","  - Time: 20.72736096382141\n","- Epoch: 3/5.\n","  - Training loss: 0.486\n","  - Test loss: 0.509\n","  - Test Accuracy: 0.835\n","  - Time: 20.818732261657715\n","- Epoch: 4/5.\n","  - Training loss: 0.473\n","  - Test loss: 0.548\n","  - Test Accuracy: 0.823\n","  - Time: 20.53863549232483\n","- Epoch: 5/5.\n","  - Training loss: 0.456\n","  - Test loss: 0.510\n","  - Test Accuracy: 0.834\n","  - Time: 20.832820177078247\n","- Total time: 119.51724076271057\n","\n","For the case of lr = 0.0001, I have obtained:\n","- Epoch: 1/5.\n","  - Training loss: 0.651\n","  - Test loss: 0.514\n","  - Test Accuracy: 0.815\n","  - Time: 20.479434728622437\n","- Epoch: 2/5.\n","  - Training loss: 0.455\n","  - Test loss: 0.460\n","  - Test Accuracy: 0.835\n","  - Time: 20.618394374847412\n","- Epoch: 3/5.\n","  - Training loss: 0.416\n","  - Test loss: 0.441\n","  - Test Accuracy: 0.841\n","  - Time: 20.535950422286987\n","- Epoch: 4/5.\n","  - Training loss: 0.392\n","  - Test loss: 0.419\n","  - Test Accuracy: 0.848\n","  - Time: 20.329606533050537\n","- Epoch: 5/5.\n","  - Training loss: 0.374\n","  - Test loss: 0.409\n","  - Test Accuracy: 0.853\n","  - Time: 20.27568006515503\n","- Total time: 118.01114583015442\n","\n","For the LR, it has quite unusual pattern. It appears that lr = 0.001 performs the best among 3, then 0.0001, and lastly 0.01. It appears for both changing in lr has impacted training loss and test loss where both losses increased in 0.01 and 0.0001 compared to lr = 0.001. In terms of time, the time to run was about the same for all three cases.\n","\n","\n","For the last case of optimizer, the change occurred in \n","- optimizer = optim.Adam(model.parameters(), lr = 0.001)\n","- From above, Adam changed to SGD.\n","\n","With the change, I have obtained the result to be\n","- Epoch: 1/5.\n","  - Training loss: 1.355\n","  - Test loss: 0.901\n","  - Test Accuracy: 0.726\n","  - Time: 19.296220779418945\n","- Epoch: 2/5.\n","  - Training loss: 0.774\n","  - Test loss: 0.716\n","  - Test Accuracy: 0.756\n","  - Time: 19.082611083984375\n","- Epoch: 3/5.\n","  - Training loss: 0.661\n","  - Test loss: 0.647\n","  - Test Accuracy: 0.772\n","  - Time: 19.03814721107483\n","- Epoch: 4/5.\n","  - Training loss: 0.607\n","  - Test loss: 0.608\n","  - Test Accuracy: 0.780\n","  - Time: 19.375463485717773\n","- Epoch: 5/5.\n","  - Training loss: 0.572\n","  - Test loss: 0.580\n","  - Test Accuracy: 0.795\n","  - Time: 19.402095079421997\n","- Total time: 112.22894525527954\n","\n","Changing from Adam to SGD has decreased the run time a little bit. However, both training and test loss increased with change to SGD, and test accuracy has decreased. \n","\n"]}]}