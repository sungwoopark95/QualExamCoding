{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49e0862-1aef-484f-a938-8b30cd9f528b",
   "metadata": {},
   "source": [
    "## SPDS HW5\n",
    "\n",
    "### Name: Jeonghyun Moon\n",
    "\n",
    "### Student ID: 2021-35791\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bad52c-51c7-4e65-810f-28aefa7aad58",
   "metadata": {},
   "source": [
    "### Problem 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d63d5d-6de9-4db8-989e-df35feb28a85",
   "metadata": {},
   "source": [
    "As an approach to this problem, I have used the code provided from Michael Nielson's book, which is provided in the following website: http://neuralnetworksanddeeplearning.com/chap1.html. Because the original code had eta instead of lr, I have updated areas in which it had lr to eta. Other than lr-eta pair, I have also updated xrange() to range() because xrange() is only in Python 2, and it was replaced with range() in Python 3. I will provide the section of the code that performs each bullent points asked in the questions.\n",
    "\n",
    "The question asks where the evaluation of the feedforward computation and input label is performed. In order to compare the two values, the evaluate(self, test_data) is used, and in the main function of SGD, it can be found right after updating the mini batch occurs, which is the following area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3280338-0612-40c2-a1e6-3b4ac7d93e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def SGD(self, training_data, epochs, mini_batch_size, lr, momentum, test_data=None):\n",
    "        ...\n",
    "        ...\n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd4f5d-d229-4a7c-927c-217af53865df",
   "metadata": {},
   "source": [
    "The above code will run if there is the test data available. It will compare tthe label and forward computed value for each epoch.\n",
    "\n",
    "Then, the next question asks where the last layer's output's gradient is calculated. This happens inside the backprop(self,x,y) algorithm. After it computes the activation layer by layer and saving z vectors for each layer, it starts from the back. It will first measure the gradient or how fast the cast is chaning as function of the output activation. The second term sigmoid_prime(zs[-1])comes from sum of all nuerons k in output layer where we have sum of k of $da_k^L$ / $dz_j^L$. Because it only depends on case when k = j, this simplifies to $da_j^L$/ $dz_j^L$ because $a_j^L$ = $\\sigma$($z_j^L$) resulting $\\sigma'$($z_j^L$). The bias is the delta value itself, and the dot product of delta and previous layers activation function would provide the nabla weight of the final layer. The code is in the following section where it computes the gradient/delta and obtain nabla for both bias and weight of the last layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cdb95-a626-487b-abfb-0d1613a5a800",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y):\n",
    "        ...\n",
    "        ...\n",
    "        # backward pass\n",
    "        # start from the back\n",
    "        # The first term self.cost_derivative(activations[-1], y) measures\n",
    "            # how fast the cost is changing as function of the output activation\n",
    "        # The second term sigmoid_prime(zs[-1]) comes from sum of all nuerons k in output layer\n",
    "            # where we have sum of k of da_k^L/ dz_j^L\n",
    "            # because it only depends on case when k = j, this simplifies to\n",
    "            # da_j^L/ dz_j^L\n",
    "            # because a_j^L = sigma(z_j^L), it become sigmoid_prime(z_j^L)\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        # This looks complicated, but there is an simple explanation\n",
    "            # since we know delta and activations, the transpose of weight matrix is applied\n",
    "            # because we can think it as moving error backward through network\n",
    "            # giving idea of measure of error at output layer \n",
    "            # (here he expects column vector so that is why transpose is applied)\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb141214-797d-44a6-b82c-d2cba503befd",
   "metadata": {},
   "source": [
    "The next qusetion asks where all the other layer's nabla for bias and weight is updated. This is right after computing the nabla for bias and weight of the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a4260a-d38c-4685-ae55-a96e4828e9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y):\n",
    "        ...\n",
    "        ...\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed320e8-096f-4c95-abf5-19346d4f5ce5",
   "metadata": {},
   "source": [
    "The code starts from 2 to number of layers using for loop. The code starts from 2, because we are starting from second to last layer. The following line z = zs[-l] makes z to be the activation value obtained earler process of back propogation. Then, sp = sigmoid_prime(z) will obtain derivative of sigmoid function. This needs to be obtained because the error term in the error is the delta, which is product of transpose of the weight matrix and error of the last layer with performing Hadamard product (HP) with derivative of activation of sigmoid function. The equation can be written as follows:\n",
    "\n",
    "$$ \\delta^l = ((w^{l+1})^T \\delta^{l+1} HP \\sigma'(z^l)$$\n",
    "\n",
    "While the above equation looks complicated, Michael Nielson provides simplar explation. Since we know delta and activations, the transpose of weight matrix is applied because we can think it as moving error backward through network giving idea of measure of error at output layer. Recall that in the matrix multiplication for forward process, we have weights * input to produce outputs, and the size of the matrix really matters since the column of the first vector must match with row of the second vector, here number of columns for the weight must equal number of rows in input to obtain the correct size for output. Note the the backward pass is actually takes in error and updates t the weight matrix and bias. Because weight is in shape of derivative of activation function and transpose of earlier activation. The weight updates are considering this item so if the weight is not tranposed, the matrix multiplication cannot be performed. \n",
    "\n",
    "The next question asks where the weight and bias itself is updated. Note the backpropogation is one that calcuates the error terms. The actual weight and bias is updated in the mini-back function in following section of the code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b871b4-0033-4e6a-9cb2-5c3cbde199cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_mini_batch(self, mini_batch, lr):\n",
    "    # TODO: Implement here\n",
    "    #pass\n",
    "    # I have used code from Michael Nielsen\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        # first initialize nabla values for b and w to be 0\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # for each mini batch\n",
    "        for x, y in mini_batch:\n",
    "            # apply backpropagation algorithm to compute the gradient of cost function\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            # update the values according to back propogation\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # update weights and biases\n",
    "        self.weights = [w-(lr/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(lr/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3f45c-b6be-4da1-899b-dea90104a0ae",
   "metadata": {},
   "source": [
    "First, we have to initial nabla b and w to be 0 because we do not know the values. Then, for each mini batch, we will perform the back propogation to obtain delta of bias and delta of b. Since updated nabla is sum of old and the delta. After the nabla values are obtained, we will use the equation to obtain updating the weights and bias which are done in the self.weights and self.biases updating in end of the end of the code (recall the updated weight is difference of old and changed value).\n",
    "\n",
    "\n",
    "When running the problem, I have obtained the following result:\n",
    "\n",
    "- Epoch 0: 3981 / 10000\n",
    "\n",
    "- Epoch 1: 6037 / 10000\n",
    "\n",
    "- Epoch 2: 6962 / 10000\n",
    "\n",
    "- Epoch 3: 7381 / 10000\n",
    "\n",
    "- Epoch 4: 7600 / 10000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ffa146-f4aa-4308-82ff-13159508ffbf",
   "metadata": {},
   "source": [
    "### Problem 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7ccd83-d2e8-46ef-bd4e-3a1765892e1c",
   "metadata": {},
   "source": [
    "Next step was doing fine tuning where if tuning is True, only the last layer's weight and bias is update. For this step, other than putting the tuning in functions, only change was made in the backpropogation where after the last layer is updated if tuning is true, it will return 0 for delta other than last layer, and if tuning is false, it will perform what was done in Problem 1. The code can be seen in the following section:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d5be6-42e5-406f-b7fb-122f432be86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y, tuning):\n",
    "        ... \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # Because last layer is finished, if tuning will end here\n",
    "        if tuning:\n",
    "            return (nabla_b, nabla_w)\n",
    "        else:\n",
    "            # Note that the variable l in the loop below is used a little\n",
    "            # differently to the notation in Chapter 2 of the book.  Here,\n",
    "            # l = 1 means the last layer of neurons, l = 2 is the\n",
    "            # second-last layer, and so on.  It's a renumbering of the\n",
    "            # scheme in the book, used here to take advantage of the fact\n",
    "            # that Python can use negative indices in lists.\n",
    "\n",
    "            for l in range(2, self.num_layers):\n",
    "                z = zs[-l]\n",
    "                sp = sigmoid_prime(z)\n",
    "                delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "                nabla_b[-l] = delta\n",
    "                nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            return (nabla_b, nabla_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce89bb59-edad-471c-9386-245844b5ae65",
   "metadata": {},
   "source": [
    "The result for running the code is as follows:\n",
    "\n",
    "- ==Full training==\n",
    "\n",
    "    - Epoch 0: 3981 / 10000\n",
    "\n",
    "    - Epoch 1: 6037 / 10000\n",
    "\n",
    "    - Epoch 2: 6962 / 10000\n",
    "    \n",
    "- ==Fine-tuning==\n",
    "    - Epoch 0: 7070 / 10000\n",
    "\n",
    "    - Epoch 1: 7171 / 10000\n",
    "\n",
    "    - Epoch 2: 7184 / 10000\n",
    "\n",
    "Note that we can see that full training is working correctly, for we have same accurracy value. Note that for fine tuning, epoch 0 was very good, but after that there were slower improvement compared to original code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc0a44-ce34-4933-ad63-82a5466e4f29",
   "metadata": {},
   "source": [
    "### Problem 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda20de-cf25-4cfb-b728-0b826a3d15ea",
   "metadata": {},
   "source": [
    "For problem 3, we were supposed to do momentum SGD. This was done through adding the velocity value for both bias and weight in initialization and updated them in the minibatch as seen in the following code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bf8ae1-f91e-4b17-bba3-e016b7f057c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, sizes):\n",
    "        ...\n",
    "        # Add velocities for weight and biases\n",
    "        self.bias_velocities = [np.zeros((y, 1)) for y in sizes[1:]]\n",
    "        self.weight_velocities = [np.zeros((y, x))\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, lr, momentum):\n",
    "        ...\n",
    "        # Update weights and biases velocities\n",
    "        self.weight_velocities = [momentum * v - (lr / len(mini_batch)) \n",
    "                                  for v in self.weight_velocities]\n",
    "        self.bias_velocities = [momentum * v - (lr / len(mini_batch))\n",
    "                                for v in self.bias_velocities]\n",
    "        \n",
    "        # update weights and biases\n",
    "        self.weights = [w + nw * v \n",
    "                        for w, nw, v in zip(self.weights, nabla_w, self.weight_velocities)]\n",
    "        self.biases = [b + nb * v\n",
    "                       for b, nb, v in zip(self.biases, nabla_b, self.bias_velocities)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864938d-6233-499e-8583-0bc1a671fe40",
   "metadata": {},
   "source": [
    "In the above code, as directed in the question, weight and bias velocities are updated first. Then, after the weight and bias velocities are updated, I have updated the weight and bias using the equation provided. When running the code, I have obtained the following result:\n",
    "\n",
    "- Epoch 0: 3979 / 10000\n",
    "\n",
    "- Epoch 1: 6043 / 10000\n",
    "\n",
    "- Epoch 2: 6964 / 10000\n",
    "\n",
    "- Epoch 3: 7379 / 10000\n",
    "\n",
    "- Epoch 4: 7599 / 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
